{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048edbcb",
   "metadata": {},
   "source": [
    "# Data for Good: predicting suicidal behavior likelihood among Reddit users using Deep Learning (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882e830",
   "metadata": {},
   "source": [
    "*Deep Learning and Reinforcement Learning (part of IBM Machine Learning Professional Certificate) - Course Project.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b55a2d",
   "metadata": {},
   "source": [
    ">*No one is useless in this world who lightens the burdens of another.*  \n",
    "â€• **Charles Dickens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6b1f4",
   "metadata": {},
   "source": [
    "<img src='https://www.discover-norway.no/upload/images/-development/header/desktop/kul_munch/edvard%20munch%20the%20scream%201893_munchmmuseet.jpg'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6cca70",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Data Preparation](#preparation)  \n",
    "2. [Model Development: Recurrent Neural Network](#model)  \n",
    "  2.1. [...](#kmeans)  \n",
    "  2.2. [...](#hac)  \n",
    "  2.3. [...](#dbscan)  \n",
    "3. [Results](#results)  \n",
    "4. [Discussion](#discussion)  \n",
    "5. [Conclusion](#conclusion)  \n",
    "  5.1. [Project Summary](#summary)  \n",
    "  5.2. [Outcome of the Analysis](#outcome)  \n",
    "  5.3. [Potential Developments](#developments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305e15f",
   "metadata": {},
   "source": [
    "## 1. Data Preparation <a name=preparation></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52342a24",
   "metadata": {},
   "source": [
    "Steps to process the data for modeling:\n",
    "1. Drop not-relevant dataset features.\n",
    "2. Remove from data the stopwords found during the word cloud analysis.\n",
    "3. Tokenize the posts.\n",
    "4. One-Hot Encode the target variable (the classes)\n",
    "5. Pad the sequences.\n",
    "6. Split dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b50ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "import keras\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randrange, seed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4affa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Post</th>\n",
       "      <th>Label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Post_nostopwords</th>\n",
       "      <th>classes</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user-0</td>\n",
       "      <td>its not a viable option and youll be leaving y...</td>\n",
       "      <td>Supportive</td>\n",
       "      <td>134</td>\n",
       "      <td>viable option leaving wife behind youd pain be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user-1</td>\n",
       "      <td>it can be hard to appreciate the notion that y...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>2163</td>\n",
       "      <td>appreciate notion meet deeply boyfriend desire...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user-2</td>\n",
       "      <td>hi so last night i was sitting on the ledge of...</td>\n",
       "      <td>Behavior</td>\n",
       "      <td>470</td>\n",
       "      <td>hi night sitting ledge window contemplating wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user-3</td>\n",
       "      <td>i tried to kill my self once and failed badly ...</td>\n",
       "      <td>Attempt</td>\n",
       "      <td>885</td>\n",
       "      <td>tried kill self failed badly cause moment want...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user-4</td>\n",
       "      <td>hi nem3030 what sorts of things do you enjoy d...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>208</td>\n",
       "      <td>hi nem3030 sorts enjoy personally welcome musi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User                                               Post       Label  \\\n",
       "0  user-0  its not a viable option and youll be leaving y...  Supportive   \n",
       "1  user-1  it can be hard to appreciate the notion that y...    Ideation   \n",
       "2  user-2  hi so last night i was sitting on the ledge of...    Behavior   \n",
       "3  user-3  i tried to kill my self once and failed badly ...     Attempt   \n",
       "4  user-4  hi nem3030 what sorts of things do you enjoy d...    Ideation   \n",
       "\n",
       "   word_count                                   Post_nostopwords  classes  \\\n",
       "0         134  viable option leaving wife behind youd pain be...        0   \n",
       "1        2163  appreciate notion meet deeply boyfriend desire...        1   \n",
       "2         470  hi night sitting ledge window contemplating wh...        1   \n",
       "3         885  tried kill self failed badly cause moment want...        1   \n",
       "4         208  hi nem3030 sorts enjoy personally welcome musi...        1   \n",
       "\n",
       "   class  \n",
       "0      0  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data (after cleaning and the EDA perfomed in word-cloud environment notebook)\n",
    "data = pd.read_csv(r'data.csv')\n",
    "processed_data = data.copy()\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a57609",
   "metadata": {},
   "source": [
    "##### 1. Drop not-relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "324e9fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post</th>\n",
       "      <th>classes</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>its not the end it just feels that way or at l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>it was a skype call but she ended it and ventr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>that sounds really weird maybe you were distra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>dont know there as dumb as it sounds i feel hy...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>gt it gets better trust me ive spent long enou...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Post  classes  class\n",
       "495  its not the end it just feels that way or at l...        0      0\n",
       "496  it was a skype call but she ended it and ventr...        0      0\n",
       "497  that sounds really weird maybe you were distra...        0      0\n",
       "498  dont know there as dumb as it sounds i feel hy...        1      1\n",
       "499  gt it gets better trust me ive spent long enou...        1      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop not relevant features\n",
    "processed_data.drop(['User', 'word_count', 'Label', 'Post_nostopwords'], axis=1, inplace=True)\n",
    "processed_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cec4a6",
   "metadata": {},
   "source": [
    "###### 2. Remove the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563a5da",
   "metadata": {},
   "source": [
    "I start processing the data by deleting the stopwords found during the word cloud analysis (see Part 1 Notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa152e87",
   "metadata": {},
   "source": [
    "#Import the stop_words list and create a Python list\n",
    "stop_words = open(r'stop_words.txt', 'r')\n",
    "sw=[]\n",
    "for line in stop_words:\n",
    "    sw.append(line[:-1])\n",
    "    \n",
    "print('Length of stop word list:', len(sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc332d61",
   "metadata": {},
   "source": [
    "#Close the file\n",
    "stop_words.close()\n",
    "print('Is the file closed?', stop_words.closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac1f1f",
   "metadata": {},
   "source": [
    "print(\"First 50 stop words:\\n\",sw[:51])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a498a1f0",
   "metadata": {},
   "source": [
    "#let's visualize a random post\n",
    "random.seed(3)\n",
    "processed_data.loc[randrange(500)]['Post']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12423c05",
   "metadata": {},
   "source": [
    "random.seed(3)\n",
    "print('Length of the post before removing the stop words:', len(processed_data.loc[randrange(500)]['Post']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6fe60",
   "metadata": {},
   "source": [
    "#let's remove the stop words\n",
    "processed_data['Post'] = processed_data['Post'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))\n",
    "\n",
    "#let's visualize the same post without stopwords\n",
    "random.seed(3)\n",
    "processed_data.loc[randrange(500)]['Post']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c98d5",
   "metadata": {},
   "source": [
    "random.seed(3)\n",
    "print('Length of the post after removing the stop words:', len(processed_data.loc[randrange(500)]['Post']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b28876",
   "metadata": {},
   "source": [
    "###### 3. Tokenize the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a9245",
   "metadata": {},
   "source": [
    "I am going to tokenize the posts, that is I'll turn the text into a list of individual words and then convert the words into integers, using the Keras Tokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d81181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dude dont do this you wont be called brave or bold you will just become the guy who killed himself a no body live through it buck up if i see it on the news when i die ill kick your ass in heaven or whever we go'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's visualize a random post\n",
    "random.seed(13)\n",
    "processed_data.loc[randrange(500)]['Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6314568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's tokenize the data\n",
    "tokenizer = Tokenizer()\n",
    "#train the tokenizer\n",
    "tokenizer.fit_on_texts(processed_data['Post'])\n",
    "#conver text into lists of integers\n",
    "posts = tokenizer.texts_to_sequences(processed_data['Post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934262e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[804, 26, 27, 28, 2, 233, 17, 561, 1243, 33, 6501, 2, 40, 23, 392, 5, 353, 78, 952, 766, 6, 63, 545, 141, 103, 7, 6502, 50, 20, 3, 101, 7, 29, 5, 1294, 57, 3, 282, 191, 1424, 14, 998, 11, 2693, 33, 11724, 85, 76]\n"
     ]
    }
   ],
   "source": [
    "#let's visualize the same post after tokenizing\n",
    "random.seed(13)\n",
    "print(posts[randrange(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0255199c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dude dont do this you wont be called brave or bold you will just become the guy who killed himself a no body live through it buck up if i see it on the news when i die ill kick your ass in heaven or whever we go'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's map the intetgers back to words to check integer meaning\n",
    "random.seed(13)\n",
    "' '.join(tokenizer.index_word[w] for w in posts[randrange(500)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a3156",
   "metadata": {},
   "source": [
    "###### 4. One-Hot Encode the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec55c6d9",
   "metadata": {},
   "source": [
    "I now one-hot encode, using Keras library, the data classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e35ad5",
   "metadata": {},
   "source": [
    "processed_data['class'] =  processed_data['Label'].apply(lambda x: 1 if x == 'Supportive' else 2 if x == 'Indicator'\n",
    "                                                         else 3 if x == 'Ideation' else 4 if x == 'Behavior' else 5 )\n",
    "\n",
    "output = keras.utils.to_categorical(processed_data['class'])\n",
    "output = output[:,1:]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79d494",
   "metadata": {},
   "source": [
    "###### 5. Pad the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a880d",
   "metadata": {},
   "source": [
    "Let's now create sequences of the same lenght. During the Exploratory Data Analysis we have foud out the 80% of posts have fewer than 2,000 words. Therefore I set the maximum sequence length as 2,000: post longer than 2,000 words will be truncated, whilst posts shorter then 2,000 words will be padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee1e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pad_sequences(posts, maxlen=2000, padding='post', truncating='post')\n",
    "#posts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b3814",
   "metadata": {},
   "source": [
    "###### 6. Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe91bf",
   "metadata": {},
   "source": [
    "Let's now create the final dataset ready for modelling, by concatenating the tokenized word sequences with the encoded classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7189c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = np.concatenate((posts, np.expand_dims(np.array(processed_data['class']), axis=1)), axis=1)\n",
    "np.shape(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c61cee",
   "metadata": {},
   "source": [
    "Let's count now the total number of words that our dataset contains. This is the size of our entire vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0403839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the pre-processing stage, the data contains 14,680 unique words\n"
     ]
    }
   ],
   "source": [
    "num_words = len(np.unique(posts))\n",
    "print('After the pre-processing stage, the data contains {} unique words'.format(f'{num_words:,}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910bca8",
   "metadata": {},
   "source": [
    "Let's split the dataset into train and test sets. I use 20% of the dataset (100 observations) as test data, and the stratify parameter to preserve the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ed919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(model_data[:,:-1], model_data[:,-1], test_size=0.2, random_state=666,\n",
    "                                                    stratify = model_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be2bd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (400, 2000)\n",
      "Testing dataset shape: (100, 2000)\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset shape:', x_train.shape)\n",
    "print('Testing dataset shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca7ee5",
   "metadata": {},
   "source": [
    "## 2. Model Development <a name= 'model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ad9d2",
   "metadata": {},
   "source": [
    "Model hyperparameters:\n",
    "- embeddeding layer dimensions and train/pretrained\n",
    "- number of layers before/after the recorrent section of the network\n",
    "- the state dimension\n",
    "- RNN initializersL default\n",
    "- number of neurons in the hidden layer(s)\n",
    "- activation functions for the hidden layers (sigmoid, tangent, relu, leaky relu)\n",
    "- learning rate\n",
    "- bach size (usually 16 or 32)\n",
    "- number of epochs\n",
    "- regularization: stochastic or mini-batch (evaluate other regularization techinque only if the model overfits the data)\n",
    "- optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe0c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Embedding, GRU\n",
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39fb05ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 300)         5315700   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 150)               67650     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 151       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,383,501\n",
      "Trainable params: 5,383,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Initialize the model\n",
    "plain_rnn = Sequential()\n",
    "\n",
    "# Add the Embedding layer, which maps each input integer (word) to a 50-dimensional vector.\n",
    "#I am not using any pre-trained embeddings\n",
    "plain_rnn.add(Embedding(posts.max()+1, output_dim=300, trainable=True, mask_zero=True))\n",
    "\n",
    "# Add the RNN layer\n",
    "plain_rnn.add(SimpleRNN(units=150, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',activation='tanh',\n",
    "                        input_shape=x_train.shape[1:]))\n",
    "\n",
    "# Add the more dense layers and the final output layer\n",
    "plain_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "plain_rnn.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#Let's check the model architecture\n",
    "plain_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee84a557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.6851 - accuracy: 0.5550 - val_loss: 0.7113 - val_accuracy: 0.5600\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.5892 - accuracy: 0.7425 - val_loss: 0.6854 - val_accuracy: 0.5700\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.5685 - accuracy: 0.7700 - val_loss: 0.6757 - val_accuracy: 0.5800\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.5193 - accuracy: 0.8825 - val_loss: 0.6645 - val_accuracy: 0.5300\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.4825 - accuracy: 0.8600 - val_loss: 0.7040 - val_accuracy: 0.5100\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.4355 - accuracy: 0.9225 - val_loss: 0.6969 - val_accuracy: 0.5400\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.3803 - accuracy: 0.9125 - val_loss: 0.7027 - val_accuracy: 0.5800\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.3263 - accuracy: 0.9425 - val_loss: 0.6976 - val_accuracy: 0.6000\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.2644 - accuracy: 0.9450 - val_loss: 0.7324 - val_accuracy: 0.5900\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.1972 - accuracy: 0.9525 - val_loss: 0.7665 - val_accuracy: 0.6100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28bf691c610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model and seed the model to get reprducible results\n",
    "plain_rnn.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae2f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.1456 - accuracy: 0.9725 - val_loss: 0.8160 - val_accuracy: 0.5800\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.1083 - accuracy: 0.9800 - val_loss: 0.8541 - val_accuracy: 0.5500\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0740 - accuracy: 0.9875 - val_loss: 0.8732 - val_accuracy: 0.5700\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0532 - accuracy: 1.0000 - val_loss: 0.9083 - val_accuracy: 0.5700\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 37s 1s/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.9657 - val_accuracy: 0.5700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28bffcac7f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model and seed the model to get reprducible results\n",
    "plain_rnn.fit(x_train, y_train, batch_size=16, epochs=5, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To output the validation set loss and metrics\n",
    "plain_rnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31b9ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e3780",
   "metadata": {},
   "source": [
    "**Gated Recurrent Unit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c66aef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 300)         5315700   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 150)               203400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 151       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,519,251\n",
      "Trainable params: 5,519,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Initialize the model\n",
    "gru_rnn = Sequential()\n",
    "\n",
    "# Add the Embedding layer, which maps each input integer (word) to a 50-dimensional vector.\n",
    "#I am not using any pre-trained embeddings\n",
    "gru_rnn.add(Embedding(posts.max()+1, output_dim=300, trainable=True, mask_zero=True))\n",
    "\n",
    "# Add the RNN layer\n",
    "gru_rnn.add(GRU(units=150, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',activation='tanh',\n",
    "                recurrent_activation=\"sigmoid\", input_shape=x_train.shape[1:]))\n",
    "\n",
    "# Add the more dense layers and the final output layer\n",
    "gru_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "gru_rnn.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#Let's check the model architecture\n",
    "gru_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f2d5804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 72s 3s/step - loss: 0.6800 - accuracy: 0.5750 - val_loss: 0.6783 - val_accuracy: 0.5900\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 61s 2s/step - loss: 0.5568 - accuracy: 0.6925 - val_loss: 0.7404 - val_accuracy: 0.5500\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 64s 3s/step - loss: 0.3491 - accuracy: 0.8900 - val_loss: 0.8835 - val_accuracy: 0.5700\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 71s 3s/step - loss: 0.1074 - accuracy: 0.9700 - val_loss: 1.1327 - val_accuracy: 0.5400\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.0238 - accuracy: 0.9950 - val_loss: 1.5161 - val_accuracy: 0.5900\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 64s 3s/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.6687 - val_accuracy: 0.6000\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 71s 3s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.8153 - val_accuracy: 0.6300\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 74s 3s/step - loss: 9.4667e-04 - accuracy: 1.0000 - val_loss: 1.8463 - val_accuracy: 0.6000\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 80s 3s/step - loss: 6.2433e-04 - accuracy: 1.0000 - val_loss: 1.8859 - val_accuracy: 0.6300\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 64s 3s/step - loss: 4.5795e-04 - accuracy: 1.0000 - val_loss: 1.9436 - val_accuracy: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b9d4c6730>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_rnn.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68654ea4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "715c209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 300)         5315700   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 150)               203400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 151       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,519,251\n",
      "Trainable params: 5,519,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Initialize the model\n",
    "gru_rnn = Sequential()\n",
    "\n",
    "# Add the Embedding layer, which maps each input integer (word) to a 50-dimensional vector.\n",
    "#I am not using any pre-trained embeddings\n",
    "gru_rnn.add(Embedding(posts.max()+1, output_dim=300, trainable=True, mask_zero=True))\n",
    "\n",
    "# Add the RNN layer\n",
    "gru_rnn.add(GRU(units=150, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',activation='tanh',\n",
    "                recurrent_activation=\"sigmoid\", input_shape=x_train.shape[1:], dropout=0.25, recurrent_dropout=0.25))\n",
    "\n",
    "# Add the more dense layers and the final output layer\n",
    "gru_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "gru_rnn.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#Let's check the model architecture\n",
    "gru_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6434e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 511s 20s/step - loss: 0.6810 - accuracy: 0.5700 - val_loss: 0.6771 - val_accuracy: 0.5900\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 558s 22s/step - loss: 0.5980 - accuracy: 0.6400 - val_loss: 0.6889 - val_accuracy: 0.5600\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 487s 20s/step - loss: 0.3979 - accuracy: 0.8600 - val_loss: 0.7916 - val_accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 495s 20s/step - loss: 0.1435 - accuracy: 0.9450 - val_loss: 0.9882 - val_accuracy: 0.6000\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 543s 22s/step - loss: 0.0379 - accuracy: 0.9925 - val_loss: 1.4460 - val_accuracy: 0.6300\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 503s 20s/step - loss: 0.0099 - accuracy: 0.9975 - val_loss: 1.5409 - val_accuracy: 0.6200\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 487s 20s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.6279 - val_accuracy: 0.6300\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 478s 19s/step - loss: 9.1180e-04 - accuracy: 1.0000 - val_loss: 1.7504 - val_accuracy: 0.6400\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 533s 21s/step - loss: 6.5760e-04 - accuracy: 1.0000 - val_loss: 1.8151 - val_accuracy: 0.6200\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 458s 18s/step - loss: 3.1995e-04 - accuracy: 1.0000 - val_loss: 1.8701 - val_accuracy: 0.6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b9d595940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_rnn.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0c538",
   "metadata": {},
   "source": [
    "Model 2: reducing complexity by reducing parameters, and using dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93214a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_1000 = pad_sequences(posts, maxlen=1000, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec8065",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = np.concatenate((posts_1000, np.expand_dims(np.array(processed_data['class']), axis=1)), axis=1)\n",
    "np.shape(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e887c84",
   "metadata": {},
   "source": [
    "Let's count now the total number of words that our dataset contains. This is the size of our entire vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(np.unique(posts_1000))\n",
    "print('After the pre-processing stage, the data contains {} unique words'.format(f'{num_words:,}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e80ecf6",
   "metadata": {},
   "source": [
    "Let's split the dataset into train and test sets. I use 20% of the dataset (100 observations) as test data, and the stratify parameter to preserve the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(model_data[:,:-1], model_data[:,-1], test_size=0.2, random_state=50,\n",
    "                                                    stratify = model_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05278b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training feature dataset shape:', x_train.shape)\n",
    "print('Testing feature dataset shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training class dataset shape:', y_train.shape)\n",
    "print('Testing class dataset shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f99412",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2)\n",
    "tensorflow.random.set_seed(2)\n",
    "\n",
    "#Initialize the model\n",
    "rnn_2 = Sequential()\n",
    "\n",
    "# Add the Embedding layer, which maps each input integer (word) to a 50-dimensional vector.\n",
    "#I am not using any pre-trained embeddings\n",
    "rnn_2.add(Embedding(posts_1000.max()+1, output_dim=250, trainable=True, mask_zero=True))\n",
    "\n",
    "# Add the RNN layer\n",
    "rnn_2.add(SimpleRNN(units=100, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', activation='tanh',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "# Add the more dense layers and the final output layer\n",
    "rnn_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "rnn_2.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#Let's check the model architecture\n",
    "rnn_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad383801",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b80775",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=5, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a375f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=5, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc8516",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9487b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = rnn_2.predict(x_train)\n",
    "sum(np.argmax(y_train, axis=1) == np.argmax(y_pred_train, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnn_2.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 0)[0]],return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "2/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc27b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual class: supportive (0)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 0)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 0)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==0)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 1)[0]],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.argmax(y_test,axis=1)==1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "6/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual class: indicator (1)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 1)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 1)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==1)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 2)[0]],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.argmax(y_test,axis=1)==2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "11/34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24444265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual class: ideation (2)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 2)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 2)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==2)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 3)[0]],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a14b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.argmax(y_test,axis=1)==3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd08da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "3/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc04603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#actual class: behavior (3)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 3)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 3)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==3)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12376b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 4)[0]],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e61d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.argmax(y_test,axis=1)==4)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "3/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual class: attempt (4)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 4)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 4)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==4)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8b5e7",
   "metadata": {},
   "source": [
    "Extras:\n",
    "1. can I do cross validation / hyperparameters tuning with deep learnig models: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/#:~:text=By%20setting%20the%20n_jobs%20argument,for%20each%20combination%20of%20parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa093d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2868ad06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dafb90e",
   "metadata": {},
   "source": [
    "sources for data-preprocessing (NLP):\n",
    "- https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "- https://medium0.com/@saad.arshad102/sentiment-analysis-text-classification-using-rnn-bi-lstm-recurrent-neural-network-81086dda8472"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450cbbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdc1f5",
   "metadata": {},
   "source": [
    "data source: https://www.kaggle.com/datasets/thedevastator/c-ssrs-labeled-suicidality-in-500-anonymized-red\n",
    "https://zenodo.org/record/2667859#.Y9aqCXZBw2z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
