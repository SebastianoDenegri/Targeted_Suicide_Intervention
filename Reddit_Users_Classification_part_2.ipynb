{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048edbcb",
   "metadata": {},
   "source": [
    "# Data for Good: predicting suicidal behavior likelihood among Reddit users using Deep Learning (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882e830",
   "metadata": {},
   "source": [
    "*Deep Learning and Reinforcement Learning (part of IBM Machine Learning Professional Certificate) - Course Project.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b55a2d",
   "metadata": {},
   "source": [
    ">*No one is useless in this world who lightens the burdens of another.*  \n",
    "â€• **Charles Dickens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6b1f4",
   "metadata": {},
   "source": [
    "<img src='https://www.discover-norway.no/upload/images/-development/header/desktop/kul_munch/edvard%20munch%20the%20scream%201893_munchmmuseet.jpg'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6cca70",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Data Preparation](#preparation)  \n",
    "2. [Model Development: Recurrent Neural Network](#model)  \n",
    "  2.1. [...](#kmeans)  \n",
    "  2.2. [...](#hac)  \n",
    "  2.3. [...](#dbscan)  \n",
    "3. [Results](#results)  \n",
    "4. [Discussion](#discussion)  \n",
    "5. [Conclusion](#conclusion)  \n",
    "  5.1. [Project Summary](#summary)  \n",
    "  5.2. [Outcome of the Analysis](#outcome)  \n",
    "  5.3. [Potential Developments](#developments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305e15f",
   "metadata": {},
   "source": [
    "## 1. Data Preparation <a name=preparation></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dac4f1",
   "metadata": {},
   "source": [
    "Steps to process the data for modeling:\n",
    "1. Drop not-relevant dataset features.\n",
    "2. Remove from data the stopwords found during the word cloud analysis.\n",
    "3. Tokenize the posts.\n",
    "4. One-Hot Encode the target variable (the classes)\n",
    "5. Pad the sequences.\n",
    "6. Split dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b50ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "import keras\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randrange, seed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4affa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Post</th>\n",
       "      <th>Label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user-0</td>\n",
       "      <td>its not a viable option and youll be leaving y...</td>\n",
       "      <td>Supportive</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user-1</td>\n",
       "      <td>it can be hard to appreciate the notion that y...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>2163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user-2</td>\n",
       "      <td>hi so last night i was sitting on the ledge of...</td>\n",
       "      <td>Behavior</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user-3</td>\n",
       "      <td>i tried to kill my self once and failed badly ...</td>\n",
       "      <td>Attempt</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user-4</td>\n",
       "      <td>hi nem3030 what sorts of things do you enjoy d...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User                                               Post       Label  \\\n",
       "0  user-0  its not a viable option and youll be leaving y...  Supportive   \n",
       "1  user-1  it can be hard to appreciate the notion that y...    Ideation   \n",
       "2  user-2  hi so last night i was sitting on the ledge of...    Behavior   \n",
       "3  user-3  i tried to kill my self once and failed badly ...     Attempt   \n",
       "4  user-4  hi nem3030 what sorts of things do you enjoy d...    Ideation   \n",
       "\n",
       "   word_count  \n",
       "0         134  \n",
       "1        2163  \n",
       "2         470  \n",
       "3         885  \n",
       "4         208  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data (after cleaning and the EDA perfomed in word-cloud environment notebook)\n",
    "data = pd.read_csv(r'data.csv')\n",
    "processed_data = data.copy()\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa936a",
   "metadata": {},
   "source": [
    "##### 1. Drop not-relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "324e9fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>its not the end it just feels that way or at l...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>it was a skype call but she ended it and ventr...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>that sounds really weird maybe you were distra...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>dont know there as dumb as it sounds i feel hy...</td>\n",
       "      <td>Attempt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>gt it gets better trust me ive spent long enou...</td>\n",
       "      <td>Behavior</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Post       Label\n",
       "495  its not the end it just feels that way or at l...  Supportive\n",
       "496  it was a skype call but she ended it and ventr...   Indicator\n",
       "497  that sounds really weird maybe you were distra...  Supportive\n",
       "498  dont know there as dumb as it sounds i feel hy...     Attempt\n",
       "499  gt it gets better trust me ive spent long enou...    Behavior"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop not relevant features\n",
    "processed_data.drop(['User', 'word_count'], axis=1, inplace=True)\n",
    "processed_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee8f73",
   "metadata": {},
   "source": [
    "###### 2. Remove the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563a5da",
   "metadata": {},
   "source": [
    "I start processing the data by deleting the stopwords found during the word cloud analysis (see Part 1 Notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9396f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of stop word list: 323\n"
     ]
    }
   ],
   "source": [
    "#Import the stop_words list and create a Python list\n",
    "stop_words = open(r'stop_words.txt', 'r')\n",
    "sw=[]\n",
    "for line in stop_words:\n",
    "    sw.append(line[:-1])\n",
    "    \n",
    "print('Length of stop word list:', len(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a109b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the file closed? True\n"
     ]
    }
   ],
   "source": [
    "#Close the file\n",
    "stop_words.close()\n",
    "print('Is the file closed?', stop_words.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e4913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 stop words:\n",
      " ['see', \"where's\", 'not', 'm', 'have', 'whom', 'need', 'maybe', 'to', 'someone', 'get', 'which', \"aren't\", 'our', 'made', 'like', \"weren't\", 'hasn', 'won', \"you'd\", \"isn't\", 'nor', 'back', \"here's\", 'my', 'else', 'too', 'shouldn', 'always', \"who's\", 'day', \"we'd\", 'how', \"how's\", 'years', 'since', 'happy', 'was', 'friends', 'under', \"i've\", 'www', 'try', 'thats', \"you'll\", 'give', 'yours', \"we've\", 'ever', 'll', 'd']\n"
     ]
    }
   ],
   "source": [
    "print(\"First 50 stop words:\\n\",sw[:51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5ba69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no more ideas i dont agree with live for others kind of advice i think you should live for yourself and your friends and family the world isnt going to be fixed any time soon so stop thinking its all on your shoulders regular exercise and a lack of excessive stress is important to a good life so is a decent job work is now stressful yes its never done im on a long break now its tired hot and humid where i now live so i cant really do anything i cant handle the heat well i want to prepare for my death before i go back to work its not only that the career enabled me to live a certain lifestyle and live in a certain place and not have to worry too much about money and other things why would you like that i dont think there are any other kinds of job i could do in this country it has been 5 years since i lost my job i have tried my best the things i lost in my life i believe them to be extremely fundamental and important things i also lost a life that had little worry and stress now i have a job that gets worse every day doesnt allow me time to exercise is in a boiling hot city that saps me of energy has horrible bitchy colleagues and so on this is the norm i have come to realize i really liked living in this country and kind of still like it other jobs will be like this or worse we live in a world of shitty jobs i had one of the best jobs in the world and threw it away i cant tolerate any job that isnt as good which is to say all of the rest of them i can move to a different job in the same industry and city in time a less hot and humid place but it wont be as good as the climate in the city where i was and even then ill still have lost years of my life people dont understand suicide and arent going to understand your suicide attempt it will just be looked upon as mental asthenia or a moment of madness or some kind of childish gesture you arent going to make people understand they dont even understand actual suicides and cant imagine why anyone would want to kill themselves i guess this lack of understanding could be a survival mechanism only suicidal people are likely to understand its not going to be fixed the world is fucked there are 7bn people fucking up the planet with our mere presence forget about it and just enjoy your life'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's visualize a random post\n",
    "random.seed(3)\n",
    "processed_data.loc[randrange(500)]['Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2318d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the post before removing the stop words: 2269\n"
     ]
    }
   ],
   "source": [
    "random.seed(3)\n",
    "print('Length of the post before removing the stop words:', len(processed_data.loc[randrange(500)]['Post']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f78c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ideas agree others kind advice family world fixed soon stop thinking shoulders regular exercise lack excessive stress important decent job stressful yes done break tired hot humid handle heat prepare death career enabled certain lifestyle certain place worry money kinds job country 5 lost job tried best lost believe extremely fundamental important lost little worry stress job gets worse allow exercise boiling hot city saps energy horrible bitchy colleagues norm realize liked living country kind jobs worse world shitty jobs best jobs world threw tolerate job rest move different job industry city less hot humid place wont climate city ill lost suicide arent suicide attempt looked upon mental asthenia moment madness kind childish gesture arent actual suicides imagine kill guess lack understanding survival mechanism suicidal likely fixed world fucked 7bn fucking planet mere presence forget enjoy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's remove the stop words\n",
    "processed_data['Post'] = processed_data['Post'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))\n",
    "\n",
    "#let's visualize the same post without stopwords\n",
    "random.seed(3)\n",
    "processed_data.loc[randrange(500)]['Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec93667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the post after removing the stop words: 904\n"
     ]
    }
   ],
   "source": [
    "random.seed(3)\n",
    "print('Length of the post after removing the stop words:', len(processed_data.loc[randrange(500)]['Post']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14249679",
   "metadata": {},
   "source": [
    "###### 3. Tokenize the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a9245",
   "metadata": {},
   "source": [
    "I am going to tokenize the posts, that is I'll turn the text into a list of individual words and then convert the words into integers, using the Keras Tokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d81181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dude wont called brave bold become guy killed body buck news die ill kick ass heaven whever'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's visualize a random post\n",
    "random.seed(13)\n",
    "processed_data.loc[randrange(500)]['Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6314568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's tokenize the data\n",
    "tokenizer = Tokenizer()\n",
    "#train the tokenizer\n",
    "tokenizer.fit_on_texts(processed_data['Post'])\n",
    "#conver text into lists of integers\n",
    "posts = tokenizer.texts_to_sequences(processed_data['Post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "934262e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[579, 39, 342, 1008, 6259, 178, 143, 725, 326, 6260, 1059, 75, 17, 1188, 770, 2454, 11488]\n"
     ]
    }
   ],
   "source": [
    "#let's visualize the same post after tokenizing\n",
    "random.seed(13)\n",
    "print(posts[randrange(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0255199c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dude wont called brave bold become guy killed body buck news die ill kick ass heaven whever'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's map the intetgers back to words to check integer meaning\n",
    "random.seed(13)\n",
    "' '.join(tokenizer.index_word[w] for w in posts[randrange(500)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52bdd06",
   "metadata": {},
   "source": [
    "###### 4. One-Hot Encode the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a93a3e",
   "metadata": {},
   "source": [
    "I now one-hot encode, using Keras library, the data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8c4d3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['class'] =  processed_data['Label'].apply(lambda x: 1 if x == 'Supportive' else 2 if x == 'Indicator'\n",
    "                                                         else 3 if x == 'Ideation' else 4 if x == 'Behavior' else 5 )\n",
    "\n",
    "output = keras.utils.to_categorical(processed_data['class'])\n",
    "output = output[:,1:]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9eaf8a",
   "metadata": {},
   "source": [
    "###### 5. Pad the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a39a0f",
   "metadata": {},
   "source": [
    "Let's now create sequences of the same lenght. During the Exploratory Data Analysis we have foud out the 80% of posts have fewer than 2,000 words. Therefore I set the maximum sequence length as 2,000: post longer than 2,000 words will be truncated, whilst posts shorter then 2,000 words will be padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6779f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pad_sequences(posts, maxlen=2000, padding='post', truncating='post')\n",
    "#posts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827e714",
   "metadata": {},
   "source": [
    "###### 6. Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de95a4c",
   "metadata": {},
   "source": [
    "Let's now create the final dataset ready for modelling, by concatenating the tokenized word sequences with the encoded classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617eb21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2005)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = np.concatenate((posts, output), axis=1)\n",
    "np.shape(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764780c",
   "metadata": {},
   "source": [
    "Let's count now the total number of words that our dataset contains. This is the size of our entire vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9375072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the pre-processing stage, the data contains 17,452 unique words\n"
     ]
    }
   ],
   "source": [
    "num_words = len(np.unique(posts))\n",
    "print('After the pre-processing stage, the data contains {} unique words'.format(f'{num_words:,}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85c994",
   "metadata": {},
   "source": [
    "Let's split the dataset into train and test sets. I use 20% of the dataset (100 observations) as test data, and the stratify parameter to preserve the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec707281",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(model_data[:,:-5], model_data[:,-5:], test_size=0.2, random_state=666,\n",
    "                                                    stratify = model_data[:,-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "752d9360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (400, 2000)\n",
      "Testing dataset shape: (100, 2000)\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset shape:', x_train.shape)\n",
    "print('Testing dataset shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b324f",
   "metadata": {},
   "source": [
    "## 2. Model Development <a name= 'model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8db1ad",
   "metadata": {},
   "source": [
    "Model hyperparameters:\n",
    "- embeddeding layer dimensions and train/pretrained\n",
    "- number of layers before/after the recorrent section of the network\n",
    "- the state dimension\n",
    "- RNN initializersL default\n",
    "- number of neurons in the hidden layer(s)\n",
    "- activation functions for the hidden layers (sigmoid, tangent, relu, leaky relu)\n",
    "- learning rate\n",
    "- bach size (usually 16 or 32)\n",
    "- number of epochs\n",
    "- regularization: stochastic or mini-batch (evaluate other regularization techinque only if the model overfits the data)\n",
    "- optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14e66392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Embedding\n",
    "from numpy.random import seed\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaa1f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 300)         5248800   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 150)               67650     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 755       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,317,205\n",
      "Trainable params: 5,317,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Initialize the model\n",
    "plain_rnn = Sequential()\n",
    "\n",
    "# Add the Embedding layer, which maps each input integer (word) to a 50-dimensional vector.\n",
    "#I am not using any pre-trained embeddings\n",
    "plain_rnn.add(Embedding(posts.max()+1, output_dim=300, trainable=True, mask_zero=True))\n",
    "\n",
    "# Add the RNN layer\n",
    "plain_rnn.add(SimpleRNN(units=150, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', activation='tanh',\n",
    "                        input_shape=x_train.shape[1:]))\n",
    "\n",
    "# Add the more dense layers and the final output layer\n",
    "#plain_rnn.add(Dense(75, activation='sigmoid'))\n",
    "#plain_rnn.add(Dense(50, activation='tanh'))\n",
    "#plain_rnn.add(Dense(25, activation='sigmoid'))\n",
    "#plain_rnn.add(Dense(10, activation='tanh'))\n",
    "plain_rnn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "plain_rnn.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#Let's check the model architecture\n",
    "plain_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "896163e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 40s 1s/step - loss: 1.6092 - accuracy: 0.2425 - val_loss: 1.5286 - val_accuracy: 0.3900\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.9782 - accuracy: 0.7625 - val_loss: 1.5176 - val_accuracy: 0.3000\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 30s 1s/step - loss: 0.3693 - accuracy: 0.9950 - val_loss: 1.5442 - val_accuracy: 0.2800\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 30s 1s/step - loss: 0.0763 - accuracy: 1.0000 - val_loss: 1.5520 - val_accuracy: 0.3000\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 1.5801 - val_accuracy: 0.3100\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 1.5861 - val_accuracy: 0.3100\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 1.6018 - val_accuracy: 0.3100\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 1.6145 - val_accuracy: 0.3000\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 1.6258 - val_accuracy: 0.3000\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.6391 - val_accuracy: 0.3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24f3764cac0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model and seed the model to get reprducible results\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)\n",
    "plain_rnn.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54a13b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 6s 442ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 2, 0, 0, 1, 1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(plain_rnn.predict(x_train), axis=1)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "264ce7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 2, 0, 0, 1, 1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_train[:10], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b385f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred == np.argmax(y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To output the validation set loss and metrics\n",
    "plain_rnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062afb52",
   "metadata": {},
   "source": [
    "Extras:\n",
    "1. can I do cross validation / hyperparameters tuning with deep learnig models: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/#:~:text=By%20setting%20the%20n_jobs%20argument,for%20each%20combination%20of%20parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868ad06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dafb90e",
   "metadata": {},
   "source": [
    "sources for data-preprocessing (NLP):\n",
    "- https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "- https://medium0.com/@saad.arshad102/sentiment-analysis-text-classification-using-rnn-bi-lstm-recurrent-neural-network-81086dda8472"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450cbbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdc1f5",
   "metadata": {},
   "source": [
    "data source: https://www.kaggle.com/datasets/thedevastator/c-ssrs-labeled-suicidality-in-500-anonymized-red\n",
    "https://zenodo.org/record/2667859#.Y9aqCXZBw2z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
