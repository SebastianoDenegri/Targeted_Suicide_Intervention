{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048edbcb",
   "metadata": {},
   "source": [
    "# Data for Good: predicting suicidal behavior likelihood among Reddit users using Deep Learning (Part 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882e830",
   "metadata": {},
   "source": [
    "*Deep Learning and Reinforcement Learning (part of IBM Machine Learning Professional Certificate) - Course Project.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b55a2d",
   "metadata": {},
   "source": [
    ">*No one is useless in this world who lightens the burdens of another.*  \n",
    "â€• **Charles Dickens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6b1f4",
   "metadata": {},
   "source": [
    "<img src='https://www.discover-norway.no/upload/images/-development/header/desktop/kul_munch/edvard%20munch%20the%20scream%201893_munchmmuseet.jpg'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6cca70",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Data Preparation](#preparation)  \n",
    "2. [Model Development: Recurrent Neural Network](#model)  \n",
    "  2.1. [...](#kmeans)  \n",
    "  2.2. [...](#hac)  \n",
    "  2.3. [...](#dbscan)  \n",
    "3. [Results](#results)  \n",
    "4. [Discussion](#discussion)  \n",
    "5. [Conclusion](#conclusion)  \n",
    "  5.1. [Project Summary](#summary)  \n",
    "  5.2. [Outcome of the Analysis](#outcome)  \n",
    "  5.3. [Potential Developments](#developments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305e15f",
   "metadata": {},
   "source": [
    "## 1. Data Preparation <a name=preparation></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52342a24",
   "metadata": {},
   "source": [
    "Steps to process the data for modeling:\n",
    "1. Drop not-relevant features.\n",
    "2. Remove stopwords from data.\n",
    "3. Split dataset into training and testing sets.\n",
    "4. Tokenize the posts.\n",
    "5. Pad the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b50ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "import keras\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randrange, seed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4affa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Post</th>\n",
       "      <th>Label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Post_nostopwords</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user-0</td>\n",
       "      <td>its not a viable option and youll be leaving y...</td>\n",
       "      <td>Supportive</td>\n",
       "      <td>134</td>\n",
       "      <td>viable option leaving wife behind youd pain be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user-1</td>\n",
       "      <td>it can be hard to appreciate the notion that y...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>2163</td>\n",
       "      <td>appreciate notion meet deeply boyfriend desire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user-2</td>\n",
       "      <td>hi so last night i was sitting on the ledge of...</td>\n",
       "      <td>Behavior</td>\n",
       "      <td>470</td>\n",
       "      <td>hi night sitting ledge window contemplating wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user-3</td>\n",
       "      <td>i tried to kill my self once and failed badly ...</td>\n",
       "      <td>Attempt</td>\n",
       "      <td>885</td>\n",
       "      <td>tried kill self failed badly cause moment want...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user-4</td>\n",
       "      <td>hi nem3030 what sorts of things do you enjoy d...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>208</td>\n",
       "      <td>hi nem3030 sorts enjoy personally welcome musi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User                                               Post       Label  \\\n",
       "0  user-0  its not a viable option and youll be leaving y...  Supportive   \n",
       "1  user-1  it can be hard to appreciate the notion that y...    Ideation   \n",
       "2  user-2  hi so last night i was sitting on the ledge of...    Behavior   \n",
       "3  user-3  i tried to kill my self once and failed badly ...     Attempt   \n",
       "4  user-4  hi nem3030 what sorts of things do you enjoy d...    Ideation   \n",
       "\n",
       "   word_count                                   Post_nostopwords  class  \n",
       "0         134  viable option leaving wife behind youd pain be...      0  \n",
       "1        2163  appreciate notion meet deeply boyfriend desire...      1  \n",
       "2         470  hi night sitting ledge window contemplating wh...      1  \n",
       "3         885  tried kill self failed badly cause moment want...      1  \n",
       "4         208  hi nem3030 sorts enjoy personally welcome musi...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data (after cleaning and the EDA perfomed in word-cloud environment notebook)\n",
    "data = pd.read_csv(r'data.csv')\n",
    "processed_data = data.copy()\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a57609",
   "metadata": {},
   "source": [
    "##### 1. Drop not-relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "324e9fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>its not the end it just feels that way or at l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>it was a skype call but she ended it and ventr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>that sounds really weird maybe you were distra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>dont know there as dumb as it sounds i feel hy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>gt it gets better trust me ive spent long enou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Post  class\n",
       "495  its not the end it just feels that way or at l...      0\n",
       "496  it was a skype call but she ended it and ventr...      0\n",
       "497  that sounds really weird maybe you were distra...      0\n",
       "498  dont know there as dumb as it sounds i feel hy...      1\n",
       "499  gt it gets better trust me ive spent long enou...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop not relevant features\n",
    "processed_data.drop(['User', 'word_count', 'Label', 'Post_nostopwords'], axis=1, inplace=True)\n",
    "processed_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cec4a6",
   "metadata": {},
   "source": [
    "###### 2. Remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106ba54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 stop words:\n",
      " ['more', \"where's\", \"they'll\", 'it', 'ought', 'whom', 'an', 'yours', 'http', \"she's\", \"they've\", 'most', 'does', \"we'd\", 'would', 'with', 'why', 'my', 'you', \"wouldn't\", 'and', 'own', 'therefore', 'but', 'i', 'do', 'get', 'her', 'into', 'could', \"haven't\", 'are', 'no', 'these', 'this', 'very', 'because', 'else', 'through', 'be', 'all', 'itself', 'www', 'otherwise', 'here', 'shall', 'k', 'same', \"don't\", \"you'd\", 'them']\n"
     ]
    }
   ],
   "source": [
    "print(\"First 50 stop words:\\n\", list(STOPWORDS)[:51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10656e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no more ideas i dont agree with live for others kind of advice i think you should live for yourself and your friends and family the world isnt going to be fixed any time soon so stop thinking its all on your shoulders regular exercise and a lack of excessive stress is important to a good life so is a decent job work is now stressful yes its never done im on a long break now its tired hot and humid where i now live so i cant really do anything i cant handle the heat well i want to prepare for my death before i go back to work its not only that the career enabled me to live a certain lifestyle and live in a certain place and not have to worry too much about money and other things why would you like that i dont think there are any other kinds of job i could do in this country it has been 5 years since i lost my job i have tried my best the things i lost in my life i believe them to be extremely fundamental and important things i also lost a life that had little worry and stress now i have a job that gets worse every day doesnt allow me time to exercise is in a boiling hot city that saps me of energy has horrible bitchy colleagues and so on this is the norm i have come to realize i really liked living in this country and kind of still like it other jobs will be like this or worse we live in a world of shitty jobs i had one of the best jobs in the world and threw it away i cant tolerate any job that isnt as good which is to say all of the rest of them i can move to a different job in the same industry and city in time a less hot and humid place but it wont be as good as the climate in the city where i was and even then ill still have lost years of my life people dont understand suicide and arent going to understand your suicide attempt it will just be looked upon as mental asthenia or a moment of madness or some kind of childish gesture you arent going to make people understand they dont even understand actual suicides and cant imagine why anyone would want to kill themselves i guess this lack of understanding could be a survival mechanism only suicidal people are likely to understand its not going to be fixed the world is fucked there are 7bn people fucking up the planet with our mere presence forget about it and just enjoy your life'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's visualize a random post\n",
    "random.seed(3)\n",
    "processed_data.loc[randrange(500)]['Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be1aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the post before removing the stop words: 2269\n"
     ]
    }
   ],
   "source": [
    "random.seed(3)\n",
    "print('Length of the post before removing the stop words:', len(processed_data.loc[randrange(500)]['Post']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f631c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's remove the stop words\n",
    "processed_data['Post']=processed_data['Post'].apply(lambda x: ' '.join([word for word in x.split() if word not in (STOPWORDS)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee25e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ideas dont agree live others kind advice think live friends family world isnt going fixed time soon stop thinking shoulders regular exercise lack excessive stress important good life decent job work now stressful yes never done im long break now tired hot humid now live cant really anything cant handle heat well want prepare death go back work career enabled live certain lifestyle live certain place worry much money things dont think kinds job country 5 years lost job tried best things lost life believe extremely fundamental important things lost life little worry stress now job gets worse every day doesnt allow time exercise boiling hot city saps energy horrible bitchy colleagues norm come realize really liked living country kind still jobs will worse live world shitty jobs one best jobs world threw away cant tolerate job isnt good say rest move different job industry city time less hot humid place wont good climate city even ill still lost years life people dont understand suicide arent going understand suicide attempt will looked upon mental asthenia moment madness kind childish gesture arent going make people understand dont even understand actual suicides cant imagine anyone want kill guess lack understanding survival mechanism suicidal people likely understand going fixed world fucked 7bn people fucking planet mere presence forget enjoy life'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(3)\n",
    "processed_data.loc[randrange(500)]['Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d63d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the post after removing the stop words: 1369\n"
     ]
    }
   ],
   "source": [
    "random.seed(3)\n",
    "print('Length of the post after removing the stop words:', len(processed_data.loc[randrange(500)]['Post']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22615121",
   "metadata": {},
   "source": [
    "###### 3. Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c03dda",
   "metadata": {},
   "source": [
    "Let's split the dataset into train and test sets. I use 20% of the dataset (100 observations) as test data, and the stratify parameter to preserve the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e9b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(processed_data['Post'], processed_data['class'], test_size=0.2,\n",
    "                                                    random_state=666, stratify = processed_data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec9eab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (400,)\n",
      "Testing dataset shape: (100,)\n",
      "Training label shape: (400,)\n",
      "Testing label shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset shape:', x_train.shape)\n",
    "print('Testing dataset shape:', x_test.shape)\n",
    "print('Training label shape:', y_train.shape)\n",
    "print('Testing label shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b28876",
   "metadata": {},
   "source": [
    "###### 3. Tokenize the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a9245",
   "metadata": {},
   "source": [
    "I am going to tokenize the posts, that is I'll turn each user's posts into a list of individual words and then convert the words into integers, using the Keras Tokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f0c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create a tokenizer object with a token for out-of-vocabulary words\n",
    "tokenizer = Tokenizer(oov_token='OOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6314568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fit the tokenizer on the training set only\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38dd8558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dont much currently lifeguard bit cash im heading school september pretty much whether something want idea currently never feel anything used make happy years happy minutes guess making things programming art stuff havent able concentrate much anything will plan something lose drive dont follow though building really closest pool 15 25 minute drive guess try going every swam much long didnt much appeal last two seasons competed last years estimated swimming somewhere around 3 million yards year dont really feel going back actually lab animals dont make feel better better shape people used swim 2 4 hours day five days week dont anymore im still physically healthy blood test nothing showed take multivitamin eat fairly well honestly dont know guess something wrong brain functions im really stressed never bee one worry much lately ive wishing way though medication death whatever medicines never worked well anti depressants ive taken done fuck guess friends finding will everyday hard let alone calling someone'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's visualize a random post from the training set\n",
    "random.seed(13)\n",
    "x_train.reset_index(inplace=False, drop=True)[randrange(400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bf411de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text into lists of integers for the training set\n",
    "x_train = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "934262e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 27,\n",
       " 671,\n",
       " 10748,\n",
       " 143,\n",
       " 3200,\n",
       " 2,\n",
       " 3495,\n",
       " 81,\n",
       " 4670,\n",
       " 145,\n",
       " 27,\n",
       " 391,\n",
       " 26,\n",
       " 9,\n",
       " 245,\n",
       " 671,\n",
       " 49,\n",
       " 8,\n",
       " 48,\n",
       " 237,\n",
       " 25,\n",
       " 80,\n",
       " 64,\n",
       " 80,\n",
       " 682,\n",
       " 211,\n",
       " 206,\n",
       " 13,\n",
       " 1967,\n",
       " 720,\n",
       " 200,\n",
       " 156,\n",
       " 114,\n",
       " 2404,\n",
       " 27,\n",
       " 48,\n",
       " 6,\n",
       " 460,\n",
       " 26,\n",
       " 372,\n",
       " 630,\n",
       " 3,\n",
       " 730,\n",
       " 71,\n",
       " 1272,\n",
       " 12,\n",
       " 1737,\n",
       " 3733,\n",
       " 763,\n",
       " 1153,\n",
       " 1410,\n",
       " 630,\n",
       " 211,\n",
       " 34,\n",
       " 22,\n",
       " 98,\n",
       " 6183,\n",
       " 27,\n",
       " 79,\n",
       " 72,\n",
       " 27,\n",
       " 2343,\n",
       " 160,\n",
       " 173,\n",
       " 5691,\n",
       " 10749,\n",
       " 160,\n",
       " 64,\n",
       " 10750,\n",
       " 2845,\n",
       " 453,\n",
       " 92,\n",
       " 324,\n",
       " 1754,\n",
       " 10751,\n",
       " 123,\n",
       " 3,\n",
       " 12,\n",
       " 8,\n",
       " 22,\n",
       " 58,\n",
       " 116,\n",
       " 10752,\n",
       " 1877,\n",
       " 3,\n",
       " 25,\n",
       " 8,\n",
       " 19,\n",
       " 19,\n",
       " 1337,\n",
       " 5,\n",
       " 237,\n",
       " 2485,\n",
       " 261,\n",
       " 481,\n",
       " 411,\n",
       " 55,\n",
       " 961,\n",
       " 155,\n",
       " 480,\n",
       " 3,\n",
       " 229,\n",
       " 2,\n",
       " 41,\n",
       " 843,\n",
       " 653,\n",
       " 1231,\n",
       " 1158,\n",
       " 89,\n",
       " 1495,\n",
       " 43,\n",
       " 10753,\n",
       " 768,\n",
       " 1273,\n",
       " 39,\n",
       " 293,\n",
       " 3,\n",
       " 4,\n",
       " 211,\n",
       " 26,\n",
       " 180,\n",
       " 417,\n",
       " 7239,\n",
       " 2,\n",
       " 12,\n",
       " 5176,\n",
       " 49,\n",
       " 10754,\n",
       " 16,\n",
       " 656,\n",
       " 27,\n",
       " 943,\n",
       " 29,\n",
       " 2254,\n",
       " 23,\n",
       " 71,\n",
       " 346,\n",
       " 217,\n",
       " 169,\n",
       " 3584,\n",
       " 49,\n",
       " 471,\n",
       " 39,\n",
       " 910,\n",
       " 1657,\n",
       " 29,\n",
       " 642,\n",
       " 164,\n",
       " 223,\n",
       " 211,\n",
       " 45,\n",
       " 368,\n",
       " 6,\n",
       " 592,\n",
       " 63,\n",
       " 83,\n",
       " 117,\n",
       " 876,\n",
       " 31]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's visualize the same post after tokenizing\n",
    "random.seed(13)\n",
    "x_train[randrange(400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0255199c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dont much currently lifeguard bit cash im heading school september pretty much whether something want idea currently never feel anything used make happy years happy minutes guess making things programming art stuff havent able concentrate much anything will plan something lose drive dont follow though building really closest pool 15 25 minute drive guess try going every swam much long didnt much appeal last two seasons competed last years estimated swimming somewhere around 3 million yards year dont really feel going back actually lab animals dont make feel better better shape people used swim 2 4 hours day five days week dont anymore im still physically healthy blood test nothing showed take multivitamin eat fairly well honestly dont know guess something wrong brain functions im really stressed never bee one worry much lately ive wishing way though medication death whatever medicines never worked well anti depressants ive taken done fuck guess friends finding will everyday hard let alone calling someone'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's map the intetgers back to words to check integer meaning\n",
    "random.seed(13)\n",
    "' '.join(tokenizer.index_word[w] for w in x_train[randrange(400)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98104cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'said mean thing earlier apologize shouldnt let jealousy better especially wanted help people live tennessee im using mostly job boards businesses around refuse speak anybody regarding employment going door door ive local unemployment office multiple times fact response every time tried online job boards try online job boards cant offer help use online job boards thing less happened went universitys employment services office dont know economic area stacks places despite efforts ive looking work past 2 months found one job one job lasted two days job im getting unemployment benefits right now im review past month say nothing fact ive searching work past 4 years ive looking different job first 3 found job actually liked wound getting laid put situation im now basically past experiences lead believe hopeless situation'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's visualize a random post from the test set\n",
    "random.seed(13)\n",
    "x_test.reset_index(inplace=False, drop=True)[randrange(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02c8de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text into lists of integers for the testing set\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5290067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86,\n",
       " 196,\n",
       " 44,\n",
       " 1610,\n",
       " 1117,\n",
       " 381,\n",
       " 83,\n",
       " 3658,\n",
       " 19,\n",
       " 322,\n",
       " 171,\n",
       " 15,\n",
       " 5,\n",
       " 68,\n",
       " 1,\n",
       " 2,\n",
       " 810,\n",
       " 728,\n",
       " 91,\n",
       " 4696,\n",
       " 4900,\n",
       " 92,\n",
       " 1928,\n",
       " 527,\n",
       " 1215,\n",
       " 2210,\n",
       " 2448,\n",
       " 22,\n",
       " 1200,\n",
       " 1200,\n",
       " 29,\n",
       " 822,\n",
       " 1783,\n",
       " 1622,\n",
       " 1321,\n",
       " 149,\n",
       " 244,\n",
       " 718,\n",
       " 98,\n",
       " 14,\n",
       " 139,\n",
       " 483,\n",
       " 91,\n",
       " 4696,\n",
       " 34,\n",
       " 483,\n",
       " 91,\n",
       " 4696,\n",
       " 33,\n",
       " 351,\n",
       " 15,\n",
       " 234,\n",
       " 483,\n",
       " 91,\n",
       " 4696,\n",
       " 44,\n",
       " 263,\n",
       " 290,\n",
       " 188,\n",
       " 8673,\n",
       " 2448,\n",
       " 1059,\n",
       " 1622,\n",
       " 3,\n",
       " 4,\n",
       " 3271,\n",
       " 640,\n",
       " 1,\n",
       " 565,\n",
       " 839,\n",
       " 2640,\n",
       " 29,\n",
       " 167,\n",
       " 46,\n",
       " 233,\n",
       " 261,\n",
       " 220,\n",
       " 197,\n",
       " 16,\n",
       " 91,\n",
       " 16,\n",
       " 91,\n",
       " 2392,\n",
       " 173,\n",
       " 155,\n",
       " 91,\n",
       " 2,\n",
       " 113,\n",
       " 1783,\n",
       " 2920,\n",
       " 38,\n",
       " 18,\n",
       " 2,\n",
       " 4428,\n",
       " 233,\n",
       " 514,\n",
       " 40,\n",
       " 89,\n",
       " 244,\n",
       " 29,\n",
       " 1541,\n",
       " 46,\n",
       " 233,\n",
       " 481,\n",
       " 64,\n",
       " 29,\n",
       " 167,\n",
       " 161,\n",
       " 91,\n",
       " 84,\n",
       " 324,\n",
       " 197,\n",
       " 91,\n",
       " 116,\n",
       " 878,\n",
       " 1640,\n",
       " 113,\n",
       " 2451,\n",
       " 147,\n",
       " 127,\n",
       " 2,\n",
       " 18,\n",
       " 466,\n",
       " 233,\n",
       " 558,\n",
       " 1030,\n",
       " 109,\n",
       " 456,\n",
       " 127]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's visualize the same post after tokenizing\n",
    "random.seed(13)\n",
    "x_test[randrange(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b510048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'said mean thing earlier apologize shouldnt let jealousy better especially wanted help people live OOV im using mostly job boards businesses around refuse speak anybody regarding employment going door door ive local unemployment office multiple times fact response every time tried online job boards try online job boards cant offer help use online job boards thing less happened went universitys employment services office dont know economic area OOV places despite efforts ive looking work past 2 months found one job one job lasted two days job im getting unemployment benefits right now im review past month say nothing fact ive searching work past 4 years ive looking different job first 3 found job actually liked wound getting laid put situation im now basically past experiences lead believe hopeless situation'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's map the intetgers back to words to check integer meaning\n",
    "random.seed(13)\n",
    "' '.join(tokenizer.index_word[w] for w in x_test[randrange(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fbfae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data contains 15,754 unique words\n"
     ]
    }
   ],
   "source": [
    "print('The training data contains {} unique words'.format(f'{len(tokenizer.word_index.items()):,}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79d494",
   "metadata": {},
   "source": [
    "###### 5. Pad the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a880d",
   "metadata": {},
   "source": [
    "Let's now create sequences of the same lenght. During the Exploratory Data Analysis we have foud out the 80% of posts have fewer than 2,000 words. Therefore I set the maximum sequence length as 2,000: post longer than 2,000 words will be truncated, whilst posts shorter then 2,000 words will be padded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4879978",
   "metadata": {},
   "source": [
    "posts = pad_sequences(posts, maxlen=2000, padding='post', truncating='post')\n",
    "#posts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca7ee5",
   "metadata": {},
   "source": [
    "## 2. Model Development <a name= 'model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae07d1",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), a word embedding is a representation of a word in the form of a vector (of a given dimenion) that encodes the meaning of the word; words that have similar meaning should also be closer in the vector space, that is they have a similar representation.  \n",
    "When working on a NLP problem, there are 2 options:\n",
    "1. Train your own word embeddings. In this case, the embeddings are learnt using the dataset for the specific problem that someone is trying to solve.\n",
    "2. Apply **Transfer Learning** concept, that is using pre-trained word embeddings, that is embeddings learnt on large datasets, saved, and then used for solving other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cc06c",
   "metadata": {},
   "source": [
    "For this project, I'll build 3 models, using the Recurrent Neural Network class;  training the models using:\n",
    "1. Learn my own word emebeddings vs using pre-trained word embeddings \n",
    "2. Simple Recurrent Neural Network\n",
    "3. Gated Recurrent Unit (GRU)\n",
    "4. Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ad9d2",
   "metadata": {},
   "source": [
    "Model hyperparameters:\n",
    "- embeddeding layer dimensions and train/pretrained\n",
    "- number of layers before/after the recorrent section of the network\n",
    "- the state dimension\n",
    "- RNN initializersL default\n",
    "- number of neurons in the hidden layer(s)\n",
    "- activation functions for the hidden layers (sigmoid, tangent, relu, leaky relu)\n",
    "- learning rate\n",
    "- bach size (usually 16 or 32)\n",
    "- number of epochs\n",
    "- regularization: stochastic or mini-batch (evaluate other regularization techinque only if the model overfits the data)\n",
    "- optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbe0c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Embedding, GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e3780",
   "metadata": {},
   "source": [
    "### 2.4. Recurrent Neural Network with Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8901b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad the sequences\n",
    "x_train_pad = pad_sequences(x_train, maxlen=100, padding='pre', truncating='pre')\n",
    "x_test_pad = pad_sequences(x_test, maxlen=100, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c66aef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 300)         4726500   \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 150)               67650     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 151       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,794,301\n",
      "Trainable params: 4,794,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#seed the model\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "#Initialize the model\n",
    "gru_rnn = Sequential()\n",
    "\n",
    "# Add the Embedding layer, which maps each input integer (word) to a 300-dimensional vector.\n",
    "gru_rnn.add(Embedding(len(tokenizer.word_index.items())+1, output_dim=300,  trainable=True))\n",
    "\n",
    "# Add the RNN layer\n",
    "gru_rnn.add(SimpleRNN(units=150, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', activation='tanh',\n",
    "                      #recurrent_activation=\"sigmoid\",\n",
    "                      input_shape=x_train_pad.shape[1:], dropout=0.4, recurrent_dropout=0.4,\n",
    "                      kernel_regularizer='l2', recurrent_regularizer='l2'))\n",
    "\n",
    "# Add the final output layer\n",
    "gru_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "gru_rnn.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#Let's check the model architecture\n",
    "gru_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f2d5804",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 6s 150ms/step - loss: 4.2688 - accuracy: 0.4725 - val_loss: 4.1952 - val_accuracy: 0.4900\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 3s 122ms/step - loss: 4.2146 - accuracy: 0.5075 - val_loss: 4.1921 - val_accuracy: 0.4300\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 3s 123ms/step - loss: 4.1995 - accuracy: 0.5275 - val_loss: 4.1854 - val_accuracy: 0.4700\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 3s 122ms/step - loss: 4.2004 - accuracy: 0.5150 - val_loss: 4.1812 - val_accuracy: 0.4700\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 3s 110ms/step - loss: 4.1406 - accuracy: 0.5800 - val_loss: 4.1587 - val_accuracy: 0.5300\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 3s 116ms/step - loss: 4.1088 - accuracy: 0.5950 - val_loss: 4.1446 - val_accuracy: 0.4900\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 3s 119ms/step - loss: 4.1490 - accuracy: 0.5500 - val_loss: 4.1418 - val_accuracy: 0.4700\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 3s 135ms/step - loss: 4.0827 - accuracy: 0.5900 - val_loss: 4.1378 - val_accuracy: 0.4400\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 4s 153ms/step - loss: 4.0609 - accuracy: 0.6075 - val_loss: 4.1112 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 4.0620 - accuracy: 0.6025 - val_loss: 4.0966 - val_accuracy: 0.4900\n"
     ]
    }
   ],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,6))\n",
    "loss = fig.add_subplot(1,2,1) \n",
    "accuracy = fig.add_subplot(1,2,2)\n",
    "\n",
    "accuracy.plot(hist.history['accuracy'])\n",
    "accuracy.plot(hist.history['val_accuracy'])\n",
    "accuracy.set_title('Model Accuracy', fontsize=13)\n",
    "accuracy.set_ylabel('Accuracy')\n",
    "accuracy.set_xlabel('Epoch')\n",
    "#accuracy.set_xticklabels(range(0,9))\n",
    "accuracy.legend(['Train set', 'Test set'], loc='best')\n",
    "accuracy.grid(True, axis='both', color='gainsboro', ls= ':', linewidth=0.8)\n",
    "\n",
    "loss.plot(hist.history['loss'])\n",
    "loss.plot(hist.history['val_loss'])\n",
    "loss.set_title('Model Error', fontsize=13)\n",
    "loss.set_ylabel('Error')\n",
    "loss.set_xlabel('Epoch')\n",
    "#loss.set_xticklabels(range(0,9))\n",
    "loss.legend(['Train set', 'Test set'], loc='best')\n",
    "loss.grid(True, axis='both', color='gainsboro', ls= ':', linewidth=0.8)\n",
    "\n",
    "plt.suptitle('Neural Network Performance', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(hist.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb190f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd31b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea974a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76859391",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e538b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b42f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8893355",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5778445",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864fba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=gru_rnn.fit(x_train_pad, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68654ea4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model\n",
    "gru_rnn = Sequential()\n",
    "\n",
    "# Add the Embedding layer, which maps each input integer (word) to a 50-dimensional vector.\n",
    "#I am not using any pre-trained embeddings\n",
    "gru_rnn.add(Embedding(posts.max()+1, output_dim=300, trainable=True, mask_zero=True))\n",
    "\n",
    "# Add the RNN layer\n",
    "gru_rnn.add(GRU(units=150, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',activation='tanh',\n",
    "                recurrent_activation=\"sigmoid\", input_shape=x_train.shape[1:], dropout=0.25, recurrent_dropout=0.25))\n",
    "\n",
    "# Add the more dense layers and the final output layer\n",
    "gru_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "gru_rnn.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#Let's check the model architecture\n",
    "gru_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_rnn.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0c538",
   "metadata": {},
   "source": [
    "Model 2: reducing complexity by reducing parameters, and using dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93214a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_1000 = pad_sequences(posts, maxlen=1000, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec8065",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = np.concatenate((posts_1000, np.expand_dims(np.array(processed_data['class']), axis=1)), axis=1)\n",
    "np.shape(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e887c84",
   "metadata": {},
   "source": [
    "Let's count now the total number of words that our dataset contains. This is the size of our entire vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(np.unique(posts_1000))\n",
    "print('After the pre-processing stage, the data contains {} unique words'.format(f'{num_words:,}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e80ecf6",
   "metadata": {},
   "source": [
    "Let's split the dataset into train and test sets. I use 20% of the dataset (100 observations) as test data, and the stratify parameter to preserve the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(model_data[:,:-1], model_data[:,-1], test_size=0.2, random_state=50,\n",
    "                                                    stratify = model_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05278b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training feature dataset shape:', x_train.shape)\n",
    "print('Testing feature dataset shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training class dataset shape:', y_train.shape)\n",
    "print('Testing class dataset shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f99412",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2)\n",
    "tensorflow.random.set_seed(2)\n",
    "\n",
    "#Initialize the model\n",
    "rnn_2 = Sequential()\n",
    "\n",
    "# Add the Embedding layer, which maps each input integer (word) to a 50-dimensional vector.\n",
    "#I am not using any pre-trained embeddings\n",
    "rnn_2.add(Embedding(posts_1000.max()+1, output_dim=250, trainable=True, mask_zero=True))\n",
    "\n",
    "# Add the RNN layer\n",
    "rnn_2.add(SimpleRNN(units=100, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', activation='tanh',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "# Add the more dense layers and the final output layer\n",
    "rnn_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "rnn_2.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#Let's check the model architecture\n",
    "rnn_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad383801",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=10, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b80775",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=5, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a375f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2.fit(x_train, y_train, batch_size=16, epochs=5, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc8516",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9487b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = rnn_2.predict(x_train)\n",
    "sum(np.argmax(y_train, axis=1) == np.argmax(y_pred_train, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnn_2.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 0)[0]],return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "2/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc27b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual class: supportive (0)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 0)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 0)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==0)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 1)[0]],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.argmax(y_test,axis=1)==1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "6/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual class: indicator (1)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 1)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 1)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==1)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 2)[0]],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.argmax(y_test,axis=1)==2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "11/34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24444265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual class: ideation (2)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 2)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 2)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==2)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 3)[0]],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a14b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.argmax(y_test,axis=1)==3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd08da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "3/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc04603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#actual class: behavior (3)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 3)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 3)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==3)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12376b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 4)[0]],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e61d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.argmax(y_test,axis=1)==4)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "3/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual class: attempt (4)\n",
    "i=0\n",
    "for v in np.unique(y_pred[np.where(np.argmax(y_test, axis=1) == 4)]):\n",
    "    print(v,\n",
    "          np.unique(y_pred[np.where(np.argmax(y_test,axis=1) == 4)[0]],return_counts=True)[1][i]/\n",
    "          len(np.where(np.argmax(y_test,axis=1)==4)[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8b5e7",
   "metadata": {},
   "source": [
    "Extras:\n",
    "1. can I do cross validation / hyperparameters tuning with deep learnig models: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/#:~:text=By%20setting%20the%20n_jobs%20argument,for%20each%20combination%20of%20parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868ad06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dafb90e",
   "metadata": {},
   "source": [
    "sources for data-preprocessing (NLP):\n",
    "- https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "- https://medium0.com/@saad.arshad102/sentiment-analysis-text-classification-using-rnn-bi-lstm-recurrent-neural-network-81086dda8472"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450cbbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdc1f5",
   "metadata": {},
   "source": [
    "data source: https://www.kaggle.com/datasets/thedevastator/c-ssrs-labeled-suicidality-in-500-anonymized-red\n",
    "https://zenodo.org/record/2667859#.Y9aqCXZBw2z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
